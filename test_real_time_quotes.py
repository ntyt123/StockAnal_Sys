#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•åçˆ¬è™«æœºåˆ¶åœ¨è·å–è‚¡ç¥¨å®æ—¶è¡Œæƒ…æ—¶çš„åº”ç”¨æƒ…å†µ
éªŒè¯æ¯ä¸ªæ•°æ®æºæ˜¯å¦æ­£ç¡®ä½¿ç”¨äº†åçˆ¬è™«ç­–ç•¥
"""

import sys
import os
import time
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_real_time_quotes_with_anti_crawler():
    """æµ‹è¯•å®æ—¶è¡Œæƒ…è·å–æ—¶çš„åçˆ¬è™«æœºåˆ¶åº”ç”¨"""
    try:
        print("å¼€å§‹æµ‹è¯•å®æ—¶è¡Œæƒ…è·å–æ—¶çš„åçˆ¬è™«æœºåˆ¶...")
        
        # æµ‹è¯•å¯¼å…¥
        from stock_select.smart_data_source import SmartStockSource
        from stock_select.anti_crawler_strategy import get_anti_crawler_strategy
        
        print("âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # è·å–å®ä¾‹
        smart_source = SmartStockSource()
        anti_crawler = get_anti_crawler_strategy()
        
        print(f"âœ… æ™ºèƒ½æ•°æ®æº: {smart_source.name}")
        print(f"âœ… åçˆ¬è™«ç­–ç•¥: {type(anti_crawler).__name__}")
        
        # æµ‹è¯•è‚¡ç¥¨åˆ—è¡¨
        print("\n1. æµ‹è¯•è‚¡ç¥¨åˆ—è¡¨è·å–:")
        try:
            stock_list = smart_source.get_stock_list()
            print(f"   âœ… æˆåŠŸè·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œå…± {len(stock_list)} åªè‚¡ç¥¨")
        except Exception as e:
            print(f"   âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
        
        # æµ‹è¯•å®æ—¶è¡Œæƒ…è·å–ï¼ˆé€ä¸ªæ•°æ®æºæµ‹è¯•ï¼‰
        print("\n2. æµ‹è¯•å®æ—¶è¡Œæƒ…è·å–ï¼ˆéªŒè¯åçˆ¬è™«æœºåˆ¶ï¼‰:")
        test_symbols = ['000001', '000002', '600000']
        
        for symbol in test_symbols:
            print(f"\n   è·å–è‚¡ç¥¨ {symbol} å®æ—¶è¡Œæƒ…...")
            start_time = time.time()
            
            try:
                # è·å–è¡Œæƒ…
                quote = smart_source.get_stock_quote(symbol)
                
                if quote and quote.get('price'):
                    print(f"   âœ… è¡Œæƒ…è·å–æˆåŠŸ")
                    print(f"      åç§°: {quote.get('name', 'N/A')}")
                    print(f"      ä»·æ ¼: {quote.get('price', 'N/A')}")
                    print(f"      æ•°æ®æº: {quote.get('source', 'N/A')}")
                else:
                    print(f"   âŒ è¡Œæƒ…æ•°æ®ä¸ºç©º")
                    
            except Exception as e:
                print(f"   âŒ è·å–å¤±è´¥: {e}")
            
            end_time = time.time()
            print(f"   è€—æ—¶: {end_time - start_time:.2f} ç§’")
        
        # æ£€æŸ¥åçˆ¬è™«ç­–ç•¥çš„ç»Ÿè®¡ä¿¡æ¯
        print("\n3. æ£€æŸ¥åçˆ¬è™«ç­–ç•¥åº”ç”¨æƒ…å†µ:")
        
        # æ£€æŸ¥è¯·æ±‚å†å²
        request_history = anti_crawler.request_history
        print(f"   è¯·æ±‚å†å²è®°å½•æ•°é‡: {len(request_history)}")
        for url, timestamp in request_history.items():
            time_str = time.strftime('%H:%M:%S', time.localtime(timestamp))
            print(f"     {url}: {time_str}")
        
        # æ£€æŸ¥æˆåŠŸæ¨¡å¼
        success_patterns = anti_crawler.get_success_patterns()
        print(f"   æˆåŠŸè¯·æ±‚æ¨¡å¼æ•°é‡: {len(success_patterns)}")
        for url, pattern in success_patterns.items():
            print(f"     {url}: æˆåŠŸ{pattern['success_count']}æ¬¡")
        
        # æ£€æŸ¥ä¼šè¯æ± 
        print(f"   ä¼šè¯æ± å¤§å°: {len(anti_crawler.session_pool)}")
        
        # æ£€æŸ¥ä»£ç†é…ç½®
        print(f"   ä»£ç†åˆ—è¡¨å¤§å°: {len(anti_crawler.proxy_list)}")
        
        # æµ‹è¯•æ‰¹é‡è·å–ï¼ˆæ¨¡æ‹Ÿå®é™…åº”ç”¨åœºæ™¯ï¼‰
        print("\n4. æµ‹è¯•æ‰¹é‡è·å–ï¼ˆéªŒè¯åçˆ¬è™«æœºåˆ¶æ•ˆæœï¼‰:")
        batch_symbols = ['000001', '000002', '000004', '000005', '000006']
        
        successful_count = 0
        total_start_time = time.time()
        
        for i, symbol in enumerate(batch_symbols, 1):
            print(f"   è¿›åº¦: {i}/{len(batch_symbols)} - è·å–è‚¡ç¥¨ {symbol}")
            
            try:
                quote = smart_source.get_stock_quote(symbol)
                if quote and quote.get('price'):
                    successful_count += 1
                    print(f"   âœ… æˆåŠŸ: {quote.get('name')} - {quote.get('price')}")
                else:
                    print(f"   âŒ å¤±è´¥: æ•°æ®ä¸ºç©º")
            except Exception as e:
                print(f"   âŒ å¤±è´¥: {e}")
        
        total_end_time = time.time()
        total_time = total_end_time - total_start_time
        
        print(f"\n   æ‰¹é‡è·å–å®Œæˆ:")
        print(f"   æˆåŠŸ: {successful_count}/{len(batch_symbols)}")
        print(f"   æˆåŠŸç‡: {successful_count/len(batch_symbols)*100:.1f}%")
        print(f"   æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print(f"   å¹³å‡æ¯åªè‚¡ç¥¨: {total_time/len(batch_symbols):.2f} ç§’")
        
        # éªŒè¯åçˆ¬è™«æœºåˆ¶æ˜¯å¦ç”Ÿæ•ˆ
        print("\n5. éªŒè¯åçˆ¬è™«æœºåˆ¶æ•ˆæœ:")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¯·æ±‚å»¶è¿Ÿ
        if len(request_history) > 1:
            print("   âœ… è¯·æ±‚å†å²è®°å½•å­˜åœ¨ï¼Œè¯´æ˜å»¶è¿Ÿæœºåˆ¶ç”Ÿæ•ˆ")
            
            # è®¡ç®—è¯·æ±‚é—´éš”
            timestamps = sorted(request_history.values())
            intervals = []
            for i in range(1, len(timestamps)):
                interval = timestamps[i] - timestamps[i-1]
                intervals.append(interval)
            
            if intervals:
                avg_interval = sum(intervals) / len(intervals)
                min_interval = min(intervals)
                print(f"   å¹³å‡è¯·æ±‚é—´éš”: {avg_interval:.2f} ç§’")
                print(f"   æœ€å°è¯·æ±‚é—´éš”: {min_interval:.2f} ç§’")
                
                if min_interval >= 0.5:  # è‡³å°‘0.5ç§’é—´éš”
                    print("   âœ… è¯·æ±‚é—´éš”åˆç†ï¼Œåçˆ¬è™«å»¶è¿Ÿæœºåˆ¶ç”Ÿæ•ˆ")
                else:
                    print("   âš ï¸  è¯·æ±‚é—´éš”è¿‡çŸ­ï¼Œå¯èƒ½å­˜åœ¨åçˆ¬è™«ç»•è¿‡é£é™©")
        else:
            print("   âš ï¸  è¯·æ±‚å†å²è®°å½•ä¸è¶³ï¼Œæ— æ³•éªŒè¯å»¶è¿Ÿæœºåˆ¶")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†å¤šä¸ªæ•°æ®æº
        if success_patterns:
            print("   âœ… æˆåŠŸæ¨¡å¼è®°å½•å­˜åœ¨ï¼Œè¯´æ˜è¯·æ±‚å¤´è½®æ¢ç­‰æœºåˆ¶ç”Ÿæ•ˆ")
        else:
            print("   âš ï¸  æˆåŠŸæ¨¡å¼è®°å½•ä¸ºç©º")
        
        print("\n" + "=" * 60)
        print("âœ… å®æ—¶è¡Œæƒ…åçˆ¬è™«æœºåˆ¶æµ‹è¯•å®Œæˆï¼")
        print("=" * 60)
        
        return successful_count > 0
        
    except ImportError as e:
        print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²åˆ›å»ºç›¸å…³æ¨¡å—æ–‡ä»¶")
        return False
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_anti_crawler_mechanisms():
    """æµ‹è¯•å…·ä½“çš„åçˆ¬è™«æœºåˆ¶"""
    try:
        print("\nå¼€å§‹æµ‹è¯•å…·ä½“åçˆ¬è™«æœºåˆ¶...")
        
        from stock_select.anti_crawler_strategy import get_anti_crawler_strategy
        
        strategy = get_anti_crawler_strategy()
        
        print("1. æµ‹è¯•è¯·æ±‚å»¶è¿Ÿæœºåˆ¶:")
        test_url = "test_url_1"
        
        # ç¬¬ä¸€æ¬¡è¯·æ±‚
        start_time = time.time()
        strategy.add_request_delay(test_url, min_delay=1.0, max_delay=2.0)
        first_request_time = time.time() - start_time
        
        # ç«‹å³ç¬¬äºŒæ¬¡è¯·æ±‚ï¼ˆåº”è¯¥è¢«å»¶è¿Ÿï¼‰
        start_time = time.time()
        strategy.add_request_delay(test_url, min_delay=1.0, max_delay=2.0)
        second_request_time = time.time() - start_time
        
        print(f"   ç¬¬ä¸€æ¬¡è¯·æ±‚è€—æ—¶: {first_request_time:.3f} ç§’")
        print(f"   ç¬¬äºŒæ¬¡è¯·æ±‚è€—æ—¶: {second_request_time:.3f} ç§’")
        
        if second_request_time > first_request_time:
            print("   âœ… è¯·æ±‚å»¶è¿Ÿæœºåˆ¶ç”Ÿæ•ˆ")
        else:
            print("   âŒ è¯·æ±‚å»¶è¿Ÿæœºåˆ¶æœªç”Ÿæ•ˆ")
        
        print("2. æµ‹è¯•ä¼šè¯è½®æ¢:")
        session1 = strategy.get_session()
        session2 = strategy.get_session()
        
        if session1 != session2:
            print("   âœ… ä¼šè¯è½®æ¢æœºåˆ¶ç”Ÿæ•ˆ")
        else:
            print("   âš ï¸  ä¼šè¯è½®æ¢æœºåˆ¶å¯èƒ½æœªç”Ÿæ•ˆ")
        
        print("3. æµ‹è¯•User-Agentè½®æ¢:")
        original_headers = dict(session1.headers)
        strategy.rotate_user_agent(session1)
        new_headers = dict(session1.headers)
        
        if original_headers != new_headers:
            print("   âœ… User-Agentè½®æ¢æœºåˆ¶ç”Ÿæ•ˆ")
        else:
            print("   âš ï¸  User-Agentè½®æ¢æœºåˆ¶å¯èƒ½æœªç”Ÿæ•ˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ åçˆ¬è™«æœºåˆ¶æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æµ‹è¯•å®æ—¶è¡Œæƒ…åçˆ¬è™«æœºåˆ¶")
    print("=" * 60)
    
    # æµ‹è¯•å®æ—¶è¡Œæƒ…åçˆ¬è™«æœºåˆ¶
    success1 = test_real_time_quotes_with_anti_crawler()
    
    # æµ‹è¯•å…·ä½“åçˆ¬è™«æœºåˆ¶
    success2 = test_anti_crawler_mechanisms()
    
    if success1 and success2:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åçˆ¬è™«æœºåˆ¶åœ¨å®æ—¶è¡Œæƒ…è·å–ä¸­æ­£å¸¸å·¥ä½œ")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
